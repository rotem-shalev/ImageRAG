<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Guiding a generative model with images as references that are retrieved dynamically to enhance the models' capabilities.">
  <meta name="keywords" content="ImageRAG, imageRAG">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src=""></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <link rel="stylesheet" href="static/css/index.css">
</head>

<body>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=7ChSzWYAAAAJ&hl=iw">Rotem Shalev-Arkushin</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://rinongal.github.io">Rinon Gal</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://www.cs.tau.ac.il/~amberman">Amit H. Bermano</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.ohadf.com">Ohad Fried</a><sup>3</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tel Aviv University,</span>
            <span class="author-block"><sup>2</sup>NVIDIA</span>
            <span class="author-block"><sup>3</sup>Reichman University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link (TODO add arxiv link in href below). -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/abs/2011.12948"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. TODO add git link below -->
              <span class="link-block">
                <a href="https://github.com/rotem-shalev/ImageRAG"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="./static/images/teaser.png">
      <p> <br> </p>
      <h2 class="subtitle has-text-centered">
        Using references broadens the generation capabilities of image generation models.
      </h2>
      <h2 class="subtitle has-text-centered">
        Given a text prompt, our method <b>ImageRAG</b>, dynamically retrieves relevant images and provides them to a base text-to-image model (T2I).
        ImageRAG works with different models, such as SDXL (A) or OmniGen (B, C), and different controls, e.g. text (A, B) or personalization (C).
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img src="./static/images/creative/origami.png">
        </div>
        <div class="item">
          <img src="./static/images/creative/african_grey.png">
        </div>
        <div class="item">
          <img src="./static/images/creative/wooden_duck.png">
        </div>
        <div class="item">
          <img src="./static/images/creative/mamba.png">
        </div>
        <div class="item">
          <img src="./static/images/creative/rhino.png">
        </div>
        <div class="item">
          <img src="./static/images/creative/raccoon_lego.png">
        </div>
        <div class="item">
          <img src="./static/images/creative/sand_taj_mahal.png">
        </div>
        <div class="item">
          <img src="./static/images/creative/nemo.png">
        </div>
        <div class="item">
          <img src="./static/images/creative/dolphins_iguazu.png">
        </div>
        <div class="item">
          <img src="./static/images/creative/recycled_sculpture.png">
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Diffusion models enable high-quality and diverse visual content synthesis.
            However, they struggle to generate rare or unseen concepts.
            To address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models.
          <br>
            We propose ImageRAG, a method that dynamically retrieves relevant images based on a given text prompt, and uses them as context to guide the generation process.
            Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation.
            In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training.
            <br>
            Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Method</h2>
        <img src="static/images/overview.png">
          <div class="content has-text-justified">
            <p>
              Given a text prompt &lt;p&gt;, we generate an initial image using a text-to-image (T2I) model.
              Then, we generate retrieval-captions &lt;c<sub>j</sub>&gt;, retrieve images from an external database for each caption &lt;i<sub>j</sub>&gt;, and use them as references to the model for better generation.
            </p>
            <p>
              Bottom part: the retrieval-caption generation block.
              We use a VLM to decide if the initial image matches the given prompt.
              If not, we ask it to list the missing concepts, and to create a caption that could be used to retrieve appropriate examples for each of these missing concepts.
            </p>
        </div>
      </div>
    </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Rare Concept Generation</h2>

        <div class="container">
      <div id="results-carousel2" class="carousel results-carousel">
        <div class="item">
          <img src="./static/images/comp/sea_lion.png">
        </div>
        <div class="item">
          <img src="./static/images/comp/duck.png">
        </div>
        <div class="item">
          <img src="./static/images/comp/chow.png">
        </div>
        <div class="item">
          <img src="./static/images/comp/geo.png">
        </div>
        <div class="item">
          <img src="./static/images/comp/bull.png">
        </div>
        <div class="item">
          <img src="./static/images/comp/jay.png">
        </div>
        <div class="item">
          <img src="./static/images/comp/academic.png">
        </div>
        <div class="item">
          <img src="./static/images/comp/cab.png">
        </div>
      </div>
    </div>
          <div class="content has-text-justified">
            <p>
              Examples of rare concept generation comparisons of SDXL and OmniGen with and without our method. <br>
              The left-most image column is the retrieved reference using ImageRAG for each prompt. <br>
              OmniGen and SDXL both struggle with the uncommon concepts,
              sometimes generating completely unrelated images. <br>
              However, when using ImageRAG, both models generate the correct concepts.
            </p>
        </div>
      </div>
    </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Evaluation</h2>
        <img class="my-image" src="static/images/eval.svg" width=100%>
          <div class="content has-text-justified">
            <p>
              Comparisons on fine-grained image generation with text-to-image models.
              We use the ImageNet (Deng et al., 2009), iNaturalist (Van Horn et al., 2018), CUB (Wah et al., 2011), and Aircraft (Maji et al., 2013) datasets.
              For each set, we report mean (± standard error) CLIP, SigLIP text-to-image similarities, and DINO similarity between real and generated images.
              Middle rows feature OmniGen-based models, while the bottom features SDXL-based models.
              In each part, best results are bolded.
            </p>
        </div>
        <br>
        <img class="my-image" src="static/images/user_study_results.png">
          <div class="content has-text-justified">
            <p>
              To further assess the quality of our results, we conduct a user study with 46 participants and a total of 767 comparisons.
              The study results are presented above with users preference percentage of our method compared to other methods,
              in terms of text alignment, visual quality, and overall preference.
              As shown, the participants favored our method, ImageRAG, over all other methods in all three criteria.
            </p>
        </div>
        <p> More experiments and ablation studies can be found in the paper, as well as more explanations about each of the above.</p>
      </div>
    </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
      <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Personalization</h2>
        <img class="my-image" src="static/images/personalization.png">
          <div class="content has-text-justified">
            <p>
            ImageRAG can work in parallel with personalization methods and enhance their capabilities.
              For example, although OmniGen can generate images of a subject based on an image, it struggles to generate some concepts.
              Using references retrieved by our method, it can generate the required result, such as my cat teaching a class of dogs, on a mug, or built from lego.
            </p>
        </div>
      </div>
    </div>
</section>

<!--TODO add bibtex-->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p>If you find our work useful, please cite our paper:</p>
    <pre><code>@article{
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io/">Nerfies</a> project page.
            If you want to reuse their <a href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
